{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ea8ba4-1e16-4b3b-b22e-e2e5c813965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총 10개의 JSON 문서를 로드했습니다.\n",
      "\n",
      "===============================\n",
      "🔍 청킹 설정: 500+50\n",
      "===============================\n",
      "\n",
      "▶️ [Top 1] 유사도: 0.6761\n",
      "로 장기발전전략 수립\n",
      "▷ 고객만족도 제고를 위한 기초조사실시의 필요성 부각\n",
      "▷ 눈높이 민원행정의 실현\n",
      "2. 연구의 목적\n",
      "횡성군 종합민원실에서 제공하는 행정서비스의 고객인 이용자들로 하여금 민원행정서비스의 전달체계와 그 서비스 과정에서 공무원들의 행태와 민원 제도, 시설 등에 대하여 체감정도를 조사를 목적으로 하고 있다. \n",
      "횡성군 민원행정 서비스 고객만족도 조사는 구체적으로 다음과 같은 의의를 내포하고 있다. \n",
      "▷ 고객중심의 초일류 민원행정서비스의 실현\n",
      "▷ 고객만족도 제고 및 고객감동 지향\n",
      "▷ 불합리한 제도개선과 발전방향 제시\n",
      "▷ 맞춤형 민원행정 서비스 실현\n",
      "▷ 전자민원 행정구현을 위한 기초조사\n",
      "▷ 행정업무 분야별 만족도 제고\n",
      "▷ 행정서비스의 취약점과 개선방향을 도출\n",
      "▷ 과거의 고객만족도 결과와 비교하여 고객만족도 수준의 변화정도를 분석고객만족도 조사는 궁극적으로 평가대상이 되는 종합민원실로 하여금 고객중심의 행정서비스를 구현하며, 행정서비스의 질을 향상시킴으로써 횡성군의 경쟁력 제고 ...\n",
      "\n",
      "\n",
      "▶️ [Top 2] 유사도: 0.6384\n",
      "제1장 연구개요\n",
      "I. 연구배경 및 목적\n",
      "1. 연구의 배경민원행정서비스에 대한 만족도 조사는 한국행정연구원에서 1996년 조사모델과 방법을 개발한 이후 현재까지 지속적으로 수행하고 있는 중앙정부 및 지방정부의 과제이다. \n",
      "횡성군 민원행정서비스 만족도조사는 횡성군 종합민원실에서 제공하는 민원행정서비스를 이용한 지역주민과 이용객을 대상으로 하여 행정서비스의 전달체계와 그 서비스 과정에서의 공무원들의 행태와 민원제도 등에 대해 체감하는 만족도를 조사하고자 실시되었다. \n",
      "특히 상반기조사결과를 비교 대상으로 하여 하반기 고객 만족도의 수준을 평가하고 발전지향적인 고객중심의 행정서비스 체제를 구축하는데 있어 자료로 활용함으로써 횡성군이 제공하는 민원행정서비스의 질적 향상에 기여하고자 한다. \n",
      "▷ 친절하고 쾌적한 민원실\n",
      "▷ 고객중심의 초일류 민원 행정기관으로서의 위상정립\n",
      "▷ 이용고객에 대한 과학적 분석을 토대로 장기발전전략 수립\n",
      "▷ 고객만족도 제고를 위한 기초조사실시의 필요성 부각\n",
      "▷ 눈높이 민원 ...\n",
      "\n",
      "\n",
      "▶️ [Top 3] 유사도: 0.5579\n",
      "금 고객중심의 행정서비스를 구현하며, 행정서비스의 질을 향상시킴으로써 횡성군의 경쟁력 제고를 궁극적인 목표로 하고 있다고 할 수 있다. \n",
      "II. 연구범위 및 방법\n",
      "1. 연구범위\n",
      "▷ 시간적 범위\n",
      "- 2006. 10. 27 ~ 2006. 11. 30(35일)\n",
      "▷ 공간적 범위\n",
      "- 횡성군 종합민원실\n",
      "▷ 내용적 범위\n",
      "- 서론(연구 개요로서 연구의 배경과 목적 및 범위 및 방법)\n",
      "- 민원 만족도 조사(민원서비스 시설, 민원업무 담당공무원의 친절도, 민원처리내용 중심)\n",
      "- 설문결과 분석\n",
      "- 상반기 조사결과와의 비교분석\n",
      "- 종합평가 및 발전방안\n",
      "2. 연구방법\n",
      "▷ 문헌연구 및 자료분석(literature study & data analysis)\n",
      "▷ 설문조사\n",
      "- 조사요원 : 2명의 설문조사원(투입전 사전교육 수료)\n",
      "- 조사기간: 2006년 11월 2일 ~ 2006년 11월 15일(10일간)\n",
      "- 조사장소: 횡성군 종합민원실\n",
      "- 조사방법: 1:1 면접식 설문조사 및 자기기입식 조사실시\n",
      "제2장 조사분석\n",
      "I ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# 1. KoSBERT 모델 로드\n",
    "model = SentenceTransformer('jhgan/ko-sbert-sts')\n",
    "\n",
    "# \n",
    "# 2. JSON 파일 읽기\n",
    "folder_path = \"./new2\"\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "raw_documents = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "        # 최상위에 \"data\" 키가 있고, 그 안에 리스트 구조가 있다고 가정\n",
    "        if \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            for item in data[\"data\"]:\n",
    "                # corpus 필드가 있는 경우에만 추가\n",
    "                if \"corpus\" in item:\n",
    "                    raw_documents.append(item[\"corpus\"])\n",
    "\n",
    "\n",
    "print(f\"✅ 총 {len(raw_documents)}개의 JSON 문서를 로드했습니다.\")\n",
    "\n",
    "# =========================\n",
    "# 3. FAISS 인덱스 생성 함수\n",
    "# =========================\n",
    "def create_faiss_index(texts, model):\n",
    "    if not texts:\n",
    "        raise ValueError(\"❌\")\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)\n",
    "    embeddings = normalize(embeddings, axis=1)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings\n",
    "\n",
    "# =========================\n",
    "# 4. 유사 문서 검색 함수\n",
    "# =========================\n",
    "def retrieve_documents(query, index, model, texts, top_k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = normalize(query_embedding, axis=1)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [(texts[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# =========================\n",
    "# 5. 청킹 및 검색 실행 함수\n",
    "# =========================\n",
    "def process_chunking_and_retrieval(chunk_size, chunk_overlap, texts, query, model):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\",  # 줄바꿈 단위가 아닌 문자 단위 청크\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    joined_text = \"\\n\".join(texts)\n",
    "    chunked_texts = text_splitter.split_text(joined_text)\n",
    "\n",
    "    if not chunked_texts:\n",
    "        raise ValueError(\"❌ 청킹된 텍스트가 없습니다.\")\n",
    "\n",
    "    index, _ = create_faiss_index(chunked_texts, model)\n",
    "    retrieved = retrieve_documents(query, index, model, chunked_texts, top_k=3)\n",
    "    return retrieved\n",
    "\n",
    "# =========================\n",
    "# 6. 질의 및 청킹 세트 정의\n",
    "# =========================\n",
    "query = \"횡성군 종합민원실의 민원행정서비스 조사는 며칠을 선정하여 실시하였어?\"\n",
    "chunking_configs = [\n",
    "    (500, 50)\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 7. 실행\n",
    "# =========================\n",
    "for chunk_size, chunk_overlap in chunking_configs:\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\"🔍 청킹 설정: {chunk_size}+{chunk_overlap}\")\n",
    "    print(f\"===============================\")\n",
    "    try:\n",
    "        results = process_chunking_and_retrieval(chunk_size, chunk_overlap, raw_documents, query, model)\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"\\n▶️ [Top {i+1}] 유사도: {score:.4f}\")\n",
    "            print(doc[:700], \"...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce083afa-efd6-459a-b390-a00e2df88d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총 10개의 JSON 문서를 로드했습니다.\n",
      "\n",
      "===============================\n",
      "🔍 MultiVectorRetriever: 청킹 500+50\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# =========================\n",
    "# 1. 모델 로드\n",
    "# =========================\n",
    "model_name = 'jhgan/ko-sbert-sts'\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# =========================\n",
    "# 2. JSON 문서 로드\n",
    "# =========================\n",
    "folder_path = \"./new2\"\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "raw_documents = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            for item in data[\"data\"]:\n",
    "                if \"corpus\" in item:\n",
    "                    raw_documents.append(item[\"corpus\"])\n",
    "\n",
    "print(f\"✅ 총 {len(raw_documents)}개의 JSON 문서를 로드했습니다.\")\n",
    "\n",
    "# =========================\n",
    "# 3. 청킹 함수\n",
    "# =========================\n",
    "def chunk_documents(texts, chunk_size, chunk_overlap):\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\",  # 문자 단위 청킹\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    full_text = \"\\n\".join(texts)\n",
    "    chunks = splitter.split_text(full_text)\n",
    "\n",
    "    documents = [\n",
    "        Document(page_content=chunk, metadata={\"doc_id\": f\"doc_{i}\"})\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "# =========================\n",
    "# 4. FAISS 인덱스 생성\n",
    "# =========================\n",
    "def create_langchain_faiss(documents, embedding_model):\n",
    "    return FAISS.from_documents(documents, embedding=embedding_model)\n",
    "\n",
    "# =========================\n",
    "# 5. MultiVectorRetriever 실행 함수 (유사도 점수 포함)\n",
    "# =========================\n",
    "def run_multivector_retrieval(chunk_size, chunk_overlap, query, texts):\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\"🔍 MultiVectorRetriever: 청킹 {chunk_size}+{chunk_overlap}\")\n",
    "    print(f\"===============================\")\n",
    "\n",
    "    documents = chunk_documents(texts, chunk_size, chunk_overlap)\n",
    "    if not documents:\n",
    "        print(\"❌ 청킹된 문서가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 1) FAISS 벡터스토어 생성\n",
    "    vectorstore = create_langchain_faiss(documents, embedding_model)\n",
    "\n",
    "    # 2) InMemory 문서 저장소\n",
    "    docstore = InMemoryStore()\n",
    "    for doc in documents:\n",
    "        doc_id = doc.metadata[\"doc_id\"]\n",
    "        docstore.mset([(doc_id, doc)])\n",
    "\n",
    "    # 3) MultiVectorRetriever 구성\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=\"doc_id\"\n",
    "    )\n",
    "\n",
    "    # -- A. MultiVectorRetriever로 상위 문서 가져오기 (점수 없음)\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # -- B. FAISS에서 직접 유사도 점수 가져오기\n",
    "    #     (vectorstore.similarity_search_with_score)\n",
    "    #     k=3으로 점수와 함께 문서 가져오기\n",
    "    results_with_score = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "    # 결과 출력\n",
    "    if not results_with_score:\n",
    "        print(\"❌ 검색 결과가 없습니다.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    print(\"\\n🔎 [유사도 점수 직접 확인 - vectorstore 기준 (Top 3)]\")\n",
    "    for i, (doc_, score) in enumerate(results_with_score):\n",
    "        print(f\"\\n▶️ [Top {i+1}] 유사도: {score:.4f}\")\n",
    "        print(doc_.page_content[:300], \"...\\n\")\n",
    "\n",
    "# =========================\n",
    "# 6. 실행\n",
    "# =========================\n",
    "query = \"횡성군 종합민원실의 민원행정서비스 조사는 며칠을 선정하여 실시하였어?\"\n",
    "chunking_configs = [\n",
    "    (500, 50)\n",
    "]\n",
    "\n",
    "for chunk_size, chunk_overlap in chunking_configs:\n",
    "    run_multivector_retrieval(chunk_size, chunk_overlap, query, raw_documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
